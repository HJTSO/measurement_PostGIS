{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Deep learning笔记3-RNN循环神经网络\n",
    "lang: zh\n",
    "date: 2017-10-18 09:18:56\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 循环神经网络（RNN）\n",
    "-------------------------------------\n",
    "\n",
    "<center>![CNN](/image/DL/3/1-1.png)</center> \n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "x是一个向量，它表示输入层的值（这里面没有画出来表示神经元节点的圆圈）；\n",
    "\n",
    "s是一个向量，它表示隐藏层的值（这里隐藏层面画了一个节点，你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；\n",
    "\n",
    "U是输入层到隐藏层的权重矩阵；\n",
    "\n",
    "o也是一个向量，它表示输出层的值；\n",
    "\n",
    "V是隐藏层到输出层的权重矩阵。\n",
    "\n",
    "W权重矩阵是隐藏层上一次的值作为这一次的输入的权重矩阵。\n",
    "\n",
    "#### 1.1. 循环神经网络的计算\n",
    "\n",
    "##### 1.1.1. 基本循环神经网络\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{o}_t&=g(V\\mathrm{s}_t)\\qquad\\qquad\\quad(输出层)\\\\\n",
    "\\mathrm{s}_t&=f(U\\mathrm{x}_t+W\\mathrm{s}_{t-1})\\qquad(隐藏层)\\\\\n",
    "\\end{align}\n",
    "\n",
    "如果反复把隐藏层带入到输出层得到：\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{o}_t&=g(V\\mathrm{s}_t)\\\\\n",
    "&=Vf(U\\mathrm{x}_t+W\\mathrm{s}_{t-1})\\\\\n",
    "&=Vf(U\\mathrm{x}_t+Wf(U\\mathrm{x}_{t-1}+W\\mathrm{s}_{t-2}))\\\\\n",
    "&=Vf(U\\mathrm{x}_t+Wf(U\\mathrm{x}_{t-1}+Wf(U\\mathrm{x}_{t-2}+W\\mathrm{s}_{t-3})))\\\\\n",
    "&=Vf(U\\mathrm{x}_t+Wf(U\\mathrm{x}_{t-1}+Wf(U\\mathrm{x}_{t-2}+Wf(U\\mathrm{x}_{t-3}+...))))\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "##### 1.1.2. 双向循环神经网络\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "<center>![CNN](/image/DL/3/1-2.png)</center> \n",
    "\n",
    "-------------------------------------\n",
    "最终的输出值取决于和。其计算方法为：\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{y}_2=g(VA_2+V'A_2')\n",
    "\\end{align}\n",
    "\n",
    "其中:\n",
    "\n",
    "\\begin{align}\n",
    "A_2&=f(WA_1+U\\mathrm{x}_2)\\\\\n",
    "A_2'&=f(W'A_3'+U'\\mathrm{x}_2)\\\\\n",
    "\\end{align}\n",
    "\n",
    "正向计算时，隐藏层的值St与St-1有关；反向计算时，隐藏层的值S't与S't+1有关；最终的输出取决于正向和反向计算的加和。双向循环神经网络的计算方法：\n",
    "\n",
    "\n",
    "\n",
    "##### 1.1.3. 深度循环神经网络\n",
    "\n",
    "\n",
    "\n",
    "#### 1.2. 循环神经网络的训练\n",
    "\n",
    "先前向传播，再反向传播，利用链式求导计算损失函数对每个权重的偏导数（梯度），然后再根据梯度下降公式更新权重w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 长短时记忆网络（LSTM）\n",
    "-------------------------------------\n",
    "\n",
    "<center>![CNN](/image/DL/2/1.png)</center> \n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "一个循环神经网络层组成。\n",
    "\n",
    "#### 2.1. 简介\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "#### 2.2. 循环神经网络的训练\n",
    "\n",
    "先前向传播，再反向传播，利用链式求导计算损失函数对每个权重的偏导数（梯度），然后再根据梯度下降公式更新权重w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 基于TensorFlow的实现（RNN/LSTM with TF）\n",
    "\n",
    "#### 3.1. TF循环神经网络的基本实现\n",
    "\n",
    "```python\n",
    "\n",
    "# Input/Image\n",
    "input = tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape=[None, image_height, image_width, color_channels])\n",
    "\n",
    "```\n",
    "#### 3.2. TF循环神经网络的基本实现\n",
    "\n",
    "#### 3.3. TF循环神经网络的基本实现\n",
    "\n",
    "#### 3.4. TF循环神经网络的实现例\n",
    "Make TV scripts using RNN(Recurrent Neural Networks) and LSTM(Long Short-Term Memory) network. Generate a new TV script with the model from TV scripts training data sets.\n",
    "RNN(Recurrent Neural Networks)及びLSTM(Long Short-Term Memory)を使用して、訓練データを用いたモデルを作成して、新しいテレビスクリプトを生成する。\n",
    "\n",
    "程序实例 - [Github Link](https://github.com/HJTSO/tv-script-generation/blob/master/dlnd_tv_script_generation.ipynb \"Title\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 词分析框架（Word Framework）\n",
    "\n",
    "#### 4.1. word2vec\n",
    "#### 4.2. seg2seq\n",
    "\n",
    "推荐阅读 - [LSTMで夏目漱石ぽい文章の生成](https://qiita.com/elm200/items/6f84d3a42eebe6c47caa \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 程序实例（Program Example）\n",
    "\n",
    "- [Github Link](https://github.com/HJTSO/tv-script-generation/blob/master/dlnd_tv_script_generation.ipynb \"Title\") \n",
    "\n",
    "### 参考资料（Reference）\n",
    "\n",
    "- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/ \"Title\") \n",
    "\n",
    "- [TensorFlow中RNN实现](https://zhuanlan.zhihu.com/p/28196873 \"Title\") \n",
    "\n",
    "- [零基础入门深度学习(5) - 循环神经网络](https://zybuluo.com/hanbingtao/note/541458 \"Title\") \n",
    "\n",
    "- [零基础入门深度学习(6) - 长短时记忆网络](https://zybuluo.com/hanbingtao/note/581764 \"Title\") \n",
    "\n",
    "- [LSTMネットワークの概要](https://qiita.com/KojiOhki/items/89cd7b69a8a6239d67ca \"Title\") \n",
    "\n",
    "- [わかるLSTM](https://qiita.com/t_Signull/items/21b82be280b46f467d1b \"Title\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
